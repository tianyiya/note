## 路由的维护

### 用户维度

从用户维度上维护路由，即需要做到如下几点

1. 维护直播间的用户列表（分布式缓存）
2. 维护用户所在的长连接
3. 查询直播间的用户列表
4. 查询用户所在的长连接

其中，可使用广播的方式，也就是不使用用户所在的长连接信息

但是，维护直播间的用户列表，以及查询直播间的用户列表，都是有点代价的

### 长连接维度

从长连接维度，即仅需要路由层维护直播间涉及到哪些长连接，而具体这台长连接上有哪些用户在这个直播间，由这个长连接维护

大体流程如下：

1. 客户端SDK发起加入直播间指令
2. 通过长连接将指令转发到业务层
3. 业务层请求路由层添加该长连接（如果没有的话，可用set）到直播间所涉及的长连接列表
4. 业务层请求该长连接添加该用户到直播间中（可使用Map<直播间ID, Set<用户SESSION>>）

相对于用户维度，维护的东西变少，性能更高

## 批量消息优化

### 应对写扩散导致的消息风暴

将消息进行秒级合并，批量下发。虽然会有短暂延迟，但体验上损失也不大

对于长轮询的方案，也可以使用基于秒级合并的方案，避免消息量大时，客户端频繁地发起fetch

### 消息压缩

压缩比与耗时综合考虑，比如使用br压缩算法

## 消息优先级

### 分级频控限速机制

大量的消息会导致客户端接收与展示的压力，且刷屏过快用户体验可不好

所以可以根据优先级对消息下发进行取舍

### 高优先级的实时性

高优先级可立即下发

### 高优先级的可靠性

可使用ack保证高优先级的可靠性

## 长连接优化

[长连接优化](../长连接优化.md)

### 礼物消息

对于重要消息，比如礼物消息，使用独立的消息通道

对于送礼物者，支付成功，可立即在客户端显示礼物特效

对于普通用户，可使用礼物消息的独立通道，但是不用保证消息必答

对于主播，需要确保能收到礼物消息，推拉结合，如果长连接异常，可使用短连接轮询查询礼物消息箱

## 历史消息

[概览](./概览.md)

## 资源评估

### QPS

在单实例保持25W长连接的情况下，单实例压测可达8Wqps的mcast稳定下发，保守按5Wqps容量评估；
多个长连接实例间，是完全的并发，可以较容易的水平扩容

### 带宽

不要超过单物理机的万兆网卡的带宽容量及全局出口带宽的容量
